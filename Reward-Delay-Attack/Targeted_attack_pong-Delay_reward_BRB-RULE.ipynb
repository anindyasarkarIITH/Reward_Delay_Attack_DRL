{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import cv2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "ENVIRONMENT = \"PongDeterministic-v4\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SAVE_MODELS = False  # Save models to file so you can test later\n",
    "MODEL_PATH = \"./pong-cnn-\"  # Models path for saving or loading\n",
    "SAVE_MODEL_INTERVAL = 10  # Save models at every X epoch\n",
    "TRAIN_MODEL = True  # Train model while playing (Make it False when testing a model)\n",
    "\n",
    "LOAD_MODEL_FROM_FILE = False #True  # Load model from file\n",
    "LOAD_FILE_EPISODE = 0 #900  # Load Xth episode from file\n",
    "\n",
    "BATCH_SIZE = 64  # Minibatch size that select randomly from mem for train nets\n",
    "MAX_EPISODE = 900 #100000  # Max episode\n",
    "MAX_STEP = 300 #100000  # Max step size for one episode\n",
    "\n",
    "MAX_MEMORY_LEN = 25000  # Max memory len\n",
    "MIN_MEMORY_LEN = 10000  # Min memory len before start train\n",
    "\n",
    "GAMMA = 0.97  # Discount rate\n",
    "ALPHA = 0.00025  # Learning rate\n",
    "EPSILON_DECAY = 0.99  # Epsilon decay rate by step\n",
    "\n",
    "RENDER_GAME_WINDOW = False #True  # Opens a new window to render\n",
    "\n",
    "\n",
    "class DuelCNN_Learner(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN with Duel Algo. https://arxiv.org/abs/1511.06581\n",
    "    \"\"\"\n",
    "    def __init__(self, h, w, output_size):\n",
    "        super(DuelCNN_Learner, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
    "\n",
    "        linear_input_size = convw * convh * 64  # Last conv layer's out sizes\n",
    "\n",
    "        # Action layer\n",
    "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.Alrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
    "        self.Alinear2 = nn.Linear(in_features=128, out_features=output_size)\n",
    "\n",
    "        # State Value layer\n",
    "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.Vlrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
    "        self.Vlinear2 = nn.Linear(in_features=128, out_features=1)  # Only 1 node\n",
    "\n",
    "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
    "        \"\"\"\n",
    "        Calcs conv layers output image sizes\n",
    "        \"\"\"\n",
    "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
    "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
    "        return next_w, next_h\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
    "\n",
    "        Ax = self.Alrelu(self.Alinear1(x))\n",
    "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
    "\n",
    "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
    "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
    "\n",
    "        q = Vx + (Ax - Ax.mean())\n",
    "\n",
    "        return q\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, environment):\n",
    "        \"\"\"\n",
    "        Hyperparameters definition for Agent\n",
    "        \"\"\"\n",
    "        # State size for breakout env. SS images (210, 160, 3). Used as input size in network\n",
    "        self.state_size_h = environment.observation_space.shape[0]\n",
    "        self.state_size_w = environment.observation_space.shape[1]\n",
    "        self.state_size_c = environment.observation_space.shape[2]\n",
    "\n",
    "        # Activation size for breakout env. Used as output size in network\n",
    "        self.action_size = environment.action_space.n\n",
    "\n",
    "        # Image pre process params\n",
    "        self.target_h = 80  # Height after process\n",
    "        self.target_w = 64  # Widht after process\n",
    "\n",
    "        self.crop_dim = [20, self.state_size_h, 0, self.state_size_w]  # Cut 20 px from top to get rid of the score table\n",
    "\n",
    "        # Trust rate to our experiences\n",
    "        self.gamma = GAMMA  # Discount coef for future predictions\n",
    "        self.alpha = ALPHA  # Learning Rate\n",
    "\n",
    "        # After many experinces epsilon will be 0.05\n",
    "        # So we will do less Explore more Exploit\n",
    "        self.epsilon = 1  # Explore or Exploit\n",
    "        self.epsilon_decay = EPSILON_DECAY  # Adaptive Epsilon Decay Rate\n",
    "        self.epsilon_minimum = 0.05  # Minimum for Explore\n",
    "\n",
    "        # Deque holds replay mem.\n",
    "        self.memory = deque(maxlen=MAX_MEMORY_LEN)\n",
    "\n",
    "        # Create two model for DDQN algorithm\n",
    "        self.online_model = DuelCNN_Learner(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model = DuelCNN_Learner(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        # Adam used as optimizer\n",
    "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha)\n",
    "\n",
    "    def preProcess(self, image):\n",
    "        \"\"\"\n",
    "        Process image crop resize, grayscale and normalize the images\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
    "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
    "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
    "        frame = frame.reshape(self.target_w, self.target_h) / 255  # Normalize\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Get state and do action\n",
    "        Two option can be selectedd if explore select random action\n",
    "        if exploit ask nnet for action\n",
    "        \"\"\"\n",
    "\n",
    "        act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
    "\n",
    "        if act_protocol == 'Explore':\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "                q_values = self.online_model.forward(state)  # (1, action_size)\n",
    "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def extract_data(self,agent):\n",
    "        \"\"\"\n",
    "        Train neural nets with replay memory\n",
    "        returns loss and max_q val predicted from online_net\n",
    "        \"\"\"\n",
    "        update = True\n",
    "        if len(agent.memory) < MIN_MEMORY_LEN:\n",
    "            loss, max_q = [0, 0]\n",
    "            update = False\n",
    "            return loss, max_q, 0,update, 0, 0, 0, 0, 0\n",
    "        # We get out minibatch and turn it to numpy array\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
    "        #print (\"~~~~\",type(state))\n",
    "        #state, action, reward, next_state, done = zip(*self.memory)\n",
    "        \n",
    "        # Concat batches in one array\n",
    "        # (np.arr, np.arr) ==> np.BIGarr\n",
    "        state = np.concatenate(state)\n",
    "        next_state = np.concatenate(next_state)\n",
    "\n",
    "        # Convert them to tensors\n",
    "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
    "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
    "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
    "        done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
    "        state_q_values_ = self.online_model(state)\n",
    "        \n",
    "        return 0, 0, state_q_values_, update, state, next_state, action, reward, done\n",
    "        \n",
    "        ###\n",
    "    def train(self, agent, state, next_state, action, reward, done):\n",
    "        # Make predictions\n",
    "        state_q_values = self.online_model(state)\n",
    "        next_states_q_values = self.online_model(next_state)\n",
    "        next_states_target_q_values = self.target_model(next_state)\n",
    "\n",
    "        # Find selected action's q_value\n",
    "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # Get indice of the max value of next_states_q_values\n",
    "        # Use that indice to get a q_value from next_states_target_q_values\n",
    "        # We use greedy for policy So it called off-policy\n",
    "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
    "        # Use Bellman function to find expected q value\n",
    "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
    "\n",
    "        # Calc loss with expected_q_value and q_value\n",
    "        loss = (selected_q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss, torch.max(state_q_values).item()\n",
    "\n",
    "    def storeResults(self, state, action, reward, nextState, done):\n",
    "        \"\"\"\n",
    "        Store every result to memory\n",
    "        \"\"\"\n",
    "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
    "\n",
    "    def adaptiveEpsilon(self):\n",
    "        \"\"\"\n",
    "        Adaptive Epsilon means every step\n",
    "        we decrease the epsilon so we do less Explore\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def compute_Q(self, state):\n",
    "        state_q_values_ = self.online_model(state)\n",
    "        return state_q_values_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import cv2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy as c\n",
    "\n",
    "from collections import deque\n",
    "#from args import get_train_args\n",
    "\n",
    "ENVIRONMENT = \"PongDeterministic-v4\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SAVE_MODELS = False  # Save models to file so you can test later\n",
    "MODEL_PATH = \"./pong-cnn-\"  # Models path for saving or loading\n",
    "SAVE_MODEL_INTERVAL = 10  # Save models at every X epoch\n",
    "TRAIN_MODEL = True  # Train model while playing (Make it False when testing a model)\n",
    "\n",
    "LOAD_MODEL_FROM_FILE = False #True  # Load model from file\n",
    "LOAD_FILE_EPISODE = 0 #900  # Load Xth episode from file\n",
    "\n",
    "BATCH_SIZE = 64  # Minibatch size that select randomly from mem for train nets\n",
    "MAX_EPISODE = 300 #100000  # Max episode\n",
    "MAX_STEP = 100000  # Max step size for one episode\n",
    "\n",
    "MAX_MEMORY_LEN = 64 #20000  # Max memory len\n",
    "MIN_MEMORY_LEN = 64 #5000  # Min memory len before start train\n",
    "\n",
    "\n",
    "MAX_REPLAY_MEMORY_LEN = 25000  # Max memory len\n",
    "MIN_REPLAY_MEMORY_LEN = 1000  # 64 Min memory len before start train\n",
    "\n",
    "GAMMA = 0.97  # Discount rate\n",
    "ALPHA = 0.00025  # Learning rate\n",
    "EPSILON_DECAY = 0.99  # Epsilon decay rate by step\n",
    "\n",
    "RENDER_GAME_WINDOW = False #True  # Opens a new window to render\n",
    "\n",
    "\n",
    "class DuelCNN_Attacker(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN with Duel Algo. https://arxiv.org/abs/1511.06581\n",
    "    \"\"\"\n",
    "    def __init__(self, info, h, w, output_size):\n",
    "        super(DuelCNN_Attacker, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
    "\n",
    "        linear_input_size = convw * convh * 64  # Last conv layer's out sizes\n",
    "\n",
    "        # Action layer\n",
    "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.Alrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
    "        ## Added info layer For Action\n",
    "        self.Ainfolayer1 = nn.Linear(in_features=128, out_features=12)\n",
    "        self.Ainforelu1 = nn.LeakyReLU()\n",
    "\n",
    "        self.Ainfolayer2 = nn.Linear(in_features=12+info, out_features=12)\n",
    "        self.Ainforelu2 = nn.LeakyReLU()\n",
    "\n",
    "        self.Alinear2 = nn.Linear(in_features=12, out_features=output_size)\n",
    "\n",
    "        # State Value layer\n",
    "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.Vlrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
    "        ## Added info layer For Value\n",
    "        self.Vinfolayer1 = nn.Linear(in_features=128, out_features=12)\n",
    "        self.Vinforelu1 = nn.LeakyReLU()\n",
    "\n",
    "        self.Vinfolayer2 = nn.Linear(in_features=12+info, out_features=12)\n",
    "        self.Vinforelu2 = nn.LeakyReLU()\n",
    "\n",
    "        self.Vlinear2 = nn.Linear(in_features=12, out_features=1)  # Only 1 node\n",
    "\n",
    "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
    "        \"\"\"\n",
    "        Calcs conv layers output image sizes\n",
    "        \"\"\"\n",
    "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
    "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
    "        return next_w, next_h\n",
    "\n",
    "    def forward(self, x, info):\n",
    "        if (info.shape[0] == 18 and x.shape[0] != 18):\n",
    "           info_in = torch.unsqueeze(torch.from_numpy(info), 0).float().to(DEVICE)        \n",
    "        else:\n",
    "          info_in = torch.from_numpy(info).float().to(DEVICE)\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
    "\n",
    "        Ax = self.Alrelu(self.Alinear1(x)) #[1,128]\n",
    "\n",
    "        Ax = self.Ainforelu1(self.Ainfolayer1(Ax)) #[1,12]\n",
    "        #print (Ax.size(), info_in.size())\n",
    "        Ax = torch.cat((Ax,info_in), 1)  #[1,29] concat layer\n",
    "        \n",
    "        Ax = self.Ainforelu2(self.Ainfolayer2(Ax))\n",
    "        \n",
    "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
    "        \n",
    "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
    "        \n",
    "        Vx = self.Vinforelu1(self.Vinfolayer1(Vx)) #[1,12]\n",
    "        \n",
    "        Vx = torch.cat((Vx,info_in), 1)  #[1,29] concat layer\n",
    "        \n",
    "        Vx = self.Vinforelu2(self.Vinfolayer2(Vx))\n",
    "        \n",
    "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
    "        \n",
    "        q = Vx + (Ax - Ax.mean())\n",
    "\n",
    "        return q\n",
    "\n",
    "\n",
    "class Attacker:\n",
    "    def __init__(self, environment, disk_size, xi_size, zeta, delta):\n",
    "        \"\"\"\n",
    "        Hyperparameters definition for Agent\n",
    "        \"\"\"\n",
    "        #args = get_train_args()\n",
    "        #self.info_dim = args.disk_size + args.xi_size\n",
    "        self.disk_size = disk_size #args.disk_size\n",
    "        self.info_dim = disk_size+xi_size #args.disk_size + args.xi_size\n",
    "\n",
    "        # State size for breakout env. SS images (210, 160, 3). Used as input size in network\n",
    "        self.state_size_h = environment.observation_space.shape[0]\n",
    "        self.state_size_w = environment.observation_space.shape[1]\n",
    "        self.state_size_c = environment.observation_space.shape[2]\n",
    "\n",
    "        # Activation size for breakout env. Used as output size in network\n",
    "        self.action_size = disk_size #args.disk_size + 1 #environment.action_space.n\n",
    "\n",
    "        # Image pre process params\n",
    "        self.target_h = 80  # Height after process\n",
    "        self.target_w = 64  # Widht after process\n",
    "\n",
    "        self.crop_dim = [20, self.state_size_h, 0, self.state_size_w]  # Cut 20 px from top to get rid of the score table\n",
    "\n",
    "        # Trust rate to our experiences\n",
    "        self.gamma = GAMMA  # Discount coef for future predictions\n",
    "        self.alpha = ALPHA  # Learning Rate\n",
    "\n",
    "        # After many experinces epsilon will be 0.05\n",
    "        # So we will do less Explore more Exploit\n",
    "        self.epsilon = 1  # Explore or Exploit\n",
    "        self.epsilon_decay = EPSILON_DECAY  # Adaptive Epsilon Decay Rate\n",
    "        self.epsilon_minimum = 0.05  # Minimum for Explore\n",
    "\n",
    "        # Deque holds replay mem.\n",
    "        self.memory = deque(maxlen=64)\n",
    "        self.replay_memory = deque(maxlen=MAX_REPLAY_MEMORY_LEN)\n",
    "\n",
    "        # Create two model for DDQN algorithm\n",
    "        self.online_model = DuelCNN_Attacker(info = self.info_dim, h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model = DuelCNN_Attacker(info = self.info_dim, h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        # Adam used as optimizer\n",
    "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha)\n",
    "\n",
    "\n",
    "        self.disk=np.zeros([disk_size,2])#disk_size\n",
    "        self.zeta=zeta\n",
    "        self.empty_positions=set([i for i in range(disk_size)])\n",
    "        self.delta=delta\n",
    "        self.disk_size=disk_size\n",
    "        self.save_list=[]\n",
    "        #self.memory=deque()\n",
    "\n",
    "    def preProcess(self, image):\n",
    "        \"\"\"\n",
    "        Process image crop resize, grayscale and normalize the images\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
    "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
    "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
    "        frame = frame.reshape(self.target_w, self.target_h) / 255  # Normalize\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def act(self, state, info):\n",
    "        \"\"\"\n",
    "        Get state and do action\n",
    "        Two option can be selectedd if explore select random action\n",
    "        if exploit ask nnet for action\n",
    "        \"\"\"\n",
    "        #act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
    "        act_protocol = 'Explore' if random.uniform(0, 1) <= 0.0 else 'Exploit'\n",
    "\n",
    "        if act_protocol == 'Explore':\n",
    "            action = random.randrange(self.action_size)\n",
    "            q_values = 0\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "                q_values = self.online_model.forward(state, info)  # (1, action_size)\n",
    "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
    "\n",
    "        return action, q_values\n",
    "\n",
    "    def train(self, subset, done):\n",
    "        if (len(self.replay_memory) < BATCH_SIZE):\n",
    "            return 0,0\n",
    "        \n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.replay_memory, BATCH_SIZE))              \n",
    "        s_list = list()\n",
    "        info_list = list()\n",
    "        reward_list =list()\n",
    "        next_state_list = list()\n",
    "        next_info_list = list()\n",
    "        action_list = list()\n",
    "        done_list = list()\n",
    "        for i in range(BATCH_SIZE):\n",
    "            s_list.append(torch.tensor(state[i][0], dtype=torch.float, device=DEVICE))\n",
    "            next_state_list.append(torch.tensor(next_state[i][0], dtype=torch.float, device=DEVICE))\n",
    "            info_list.append(state[i][1])\n",
    "            reward_list.append(reward[i])\n",
    "            next_info_list.append(next_state[i][1])\n",
    "            action_list.append(torch.tensor(action[i], dtype=torch.float, device=DEVICE))\n",
    "            done_list.append(done[i])\n",
    "            \n",
    "        info_attacker = np.stack(info_list, axis = 0)\n",
    "        next_info_attacker = np.stack(next_info_list, axis = 0)\n",
    "        state = torch.stack(s_list)\n",
    "        next_state = torch.stack(next_state_list)\n",
    "        reward_attacker = torch.stack(reward_list, axis = 0)\n",
    "        action = torch.stack(action_list)\n",
    "        done_attacker = torch.stack(done_list)\n",
    "        \n",
    "        \n",
    "        # Convert them to tensors\n",
    "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
    "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
    "        reward = torch.tensor(reward_attacker, dtype=torch.float, device=DEVICE)\n",
    "        done = torch.tensor(done_attacker, dtype=torch.float, device=DEVICE)\n",
    "   \n",
    "        # Make predictions\n",
    "        state_q_values = self.online_model(state,info_attacker)\n",
    "        \n",
    "        next_states_q_values = self.online_model(next_state,next_info_attacker)\n",
    "        \n",
    "        next_states_target_q_values = self.target_model(next_state,next_info_attacker)\n",
    "        \n",
    "        # Find selected action's q_value\n",
    "        \n",
    "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Get indice of the max value of next_states_q_values\n",
    "        # Use that indice to get a q_value from next_states_target_q_values\n",
    "        # We use greedy for policy So it called off-policy\n",
    "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
    "        # Use Bellman function to find expected q value\n",
    "        \n",
    "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
    "\n",
    "        #print (selected_q_value.shape, expected_q_value.shape)\n",
    "        # Calc loss with expected_q_value and q_value\n",
    "        loss = (selected_q_value - expected_q_value.detach()).pow(2).mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss, torch.max(state_q_values).item()\n",
    "        \n",
    "    def storeResults(self, state, action, reward, nextState, done):\n",
    "        \"\"\"\n",
    "        Store every result to memory\n",
    "        \"\"\"\n",
    "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
    "\n",
    "    def adaptiveEpsilon(self):\n",
    "        \"\"\"\n",
    "        Adaptive Epsilon means every step\n",
    "        we decrease the epsilon so we do less Explore\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    ### attacker MDP relatd function\n",
    "    def return_memory(self):\n",
    "        if len(self.memory)>0:\n",
    "            memory=self.memory\n",
    "            #self.memory=deque()\n",
    "            return memory\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __retrieve_infor__(self,xi):\n",
    "        \n",
    "        index=self.empty_positions.pop()\n",
    "        retrieve_reward=xi[-1]\n",
    "        self.disk[index,0]=retrieve_reward\n",
    "        self.disk[index,1]=0\n",
    "        return index,retrieve_reward\n",
    "\n",
    "    def __delay_attack_strategy__(self,obs_attacker,action):\n",
    "        if (action==0):\n",
    "            send_back_index = np.argmax(self.disk[:,-2])+1\n",
    "            return send_back_index\n",
    "        else :\n",
    "            send_back_index = np.argmin(self.disk[:,-2])+1\n",
    "            return send_back_index\n",
    "        \n",
    "    def step_one(self, args, state, next_state, action, reward, done):\n",
    "        poison_reward = reward  \n",
    "        obs_attacker = list()\n",
    "        xi = np.zeros(7)\n",
    "        state_attacker = state\n",
    "        obs_attacker.append(state_attacker)\n",
    "        xi[-1] = reward\n",
    "        xi[action] = 1\n",
    "        if len(self.save_list)!=0:\n",
    "            change=self.save_list.pop(-1)\n",
    "            reward_save = 0 # temporary reward\n",
    "            self.save_list.append(reward_save)\n",
    "        \n",
    "        index,retrieve_reward=self.__retrieve_infor__(xi)\n",
    "        info_attacker=c(np.concatenate([xi,self.disk[:,0]]))\n",
    "        obs_attacker.append(info_attacker)\n",
    "        self.save_list.append(obs_attacker)\n",
    "        #save needed information for training attacker agent\n",
    "        if len(self.save_list)==4:\n",
    "            self.memory.append(tuple(self.save_list))\n",
    "            self.save_list=[c(obs_attacker)]\n",
    "\n",
    "        send_back_index =self.__delay_attack_strategy__(obs_attacker,action)\n",
    "        \"\"\"\n",
    "        If the model predict None or disk position without information, send back None\n",
    "        \"\"\"\n",
    "        #if send_back_index==0 or send_back_index-1 in self.empty_positions:\n",
    "        if send_back_index-1 in self.empty_positions: \n",
    "            send_back_reward=None\n",
    "        else:\n",
    "            \n",
    "            send_back_reward=self.disk[send_back_index-1,-2]\n",
    "            self.empty_positions.add(send_back_index-1)\n",
    "            \n",
    "        for j in range(self.disk_size):\n",
    "            if j not in self.empty_positions:\n",
    "                self.disk[j,-1]=self.disk[j,-1]+1\n",
    "\n",
    "        self.save_list.append(send_back_index-1)\n",
    "        change=send_back_reward!=retrieve_reward\n",
    "        self.save_list.append(change)\n",
    "        if send_back_reward is not None:\n",
    "            return send_back_reward\n",
    "        else:\n",
    "            return None  \n",
    "    \n",
    "    def __dynamic_reward__(self, state_q_values_prev,state_q_values_now, act_teacher):\n",
    "        ''' uncomment the following commented lines for untargeted attack and also commented out the following uncommented lines\n",
    "        updated_policy = torch.softmax(state_q_values_now, dim=-1) #(PI_(t+1))\n",
    "        reward_attacker_ = - (torch.sum(torch.mul(updated_policy, state_q_values_prev)))\n",
    "        #reward_attacker_ = - (self.pearsonr(state_q_values_now,state_q_values_prev))\n",
    "        return reward_attacker_\n",
    "        '''\n",
    "        # compute CE LAST\n",
    "        loss_CE = torch.nn.CrossEntropyLoss()\n",
    "        target_q_value_tensor = torch.zeros(int(state_q_values_prev.shape[0]), device = DEVICE)\n",
    "        index = torch.tensor([0]).cuda() #torch.tensor([0,2]).cuda()#No Operation, Fire, Right, Left, Right Fire, Left Fire\n",
    "        target_q_value_tensor.index_fill_(0, index, 1).cuda()\n",
    "        target_q_value_dist_sq = target_q_value_tensor #(1 / 2) * target_q_value_tensor\n",
    "        target_q_value_dist = torch.unsqueeze(target_q_value_dist_sq,0)\n",
    "        prev_CE_loss = loss_CE(state_q_values_prev.unsqueeze(0), target_q_value_dist.cuda())\n",
    "        \n",
    "        # COMPUTE CE NOW\n",
    "        now_CE_loss = loss_CE(state_q_values_now.unsqueeze(0), target_q_value_dist.cuda())\n",
    "        reward_attacker_targeted = (prev_CE_loss - now_CE_loss)\n",
    "        if (reward_attacker_targeted > 0):\n",
    "            reward_attacker_ = torch.tensor([1]).cuda()\n",
    "            return reward_attacker_\n",
    "        else:\n",
    "            reward_attacker_ = torch.tensor([-1]).cuda()\n",
    "            return reward_attacker_\n",
    "        \n",
    "    def pearsonr(self,x, y):\n",
    "        mean_x = torch.mean(x)\n",
    "        mean_y = torch.mean(y)\n",
    "        xm = x.sub(mean_x)\n",
    "        ym = y.sub(mean_y)\n",
    "        r_num = xm.dot(ym)\n",
    "        r_den = torch.norm(xm, 2) * torch.norm(ym, 2)\n",
    "        r_val = r_num / r_den\n",
    "        return r_val\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import gym\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy as c\n",
    "import collections\n",
    "import itertools\n",
    "from collections import deque\n",
    "\n",
    "#from model_pong import Agent\n",
    "\n",
    "\n",
    "ENVIRONMENT = \"PongDeterministic-v4\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SAVE_MODELS = False  # Save models to file so you can test later\n",
    "MODEL_PATH = \"./pong-cnn_D_RULE\"  # Models path for saving or loading\n",
    "Teacher_path = \"pong-cnn-\"\n",
    "SAVE_MODEL_INTERVAL = 50  # Save models at every X epoch\n",
    "TRAIN_MODEL = True  # Train model while playing (Make it False when testing a model)\n",
    "\n",
    "LOAD_MODEL_FROM_FILE = False  # Load model from file\n",
    "LOAD_FILE_EPISODE_Teacher = 800 #900  # Load Xth episode from file\n",
    "LOAD_FILE_EPISODE = 0 #300\n",
    "\n",
    "BATCH_SIZE = 64  # Minibatch size that select randomly from mem for train nets\n",
    "MAX_EPISODE = 1201 #100000  # Max episode\n",
    "MAX_STEP = 100000  # Max step size for one episode\n",
    "\n",
    "MAX_MEMORY_LEN = 25000  # Max memory len\n",
    "MIN_MEMORY_LEN = 10000 #40000  # Min memory len before start train\n",
    "\n",
    "GAMMA = 0.97  # Discount rate\n",
    "ALPHA = 0.00025  # Learning rate\n",
    "EPSILON_DECAY = 0.99  # Epsilon decay rate by step\n",
    "\n",
    "RENDER_GAME_WINDOW = False #True  # Opens a new window to render the game (Won't work on colab default)\n",
    "\n",
    "\n",
    "def prep_batch_data(state, action, reward, next_state, done):\n",
    "    #state = np.concatenate(state)\n",
    "    #next_state = np.concatenate(next_state)\n",
    "\n",
    "    # Convert them to tensors\n",
    "    state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
    "    action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
    "    reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
    "    done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
    "    return state, action, reward, next_state, done\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    environment = gym.make(ENVIRONMENT)  # Get env   \n",
    "    agent = Agent(environment)  # Create Agent\n",
    "    attacker = Attacker(environment,11,7,15,1) \n",
    "    \n",
    "    ## Initialize and load weight for pretrained Teacher Network\n",
    "    teacher = c(agent) #.copy()\n",
    "    teacher.online_model.load_state_dict(torch.load(Teacher_path+str(LOAD_FILE_EPISODE_Teacher)+\".pkl\"))\n",
    "    with open(Teacher_path+str(LOAD_FILE_EPISODE_Teacher)+'.json') as outfile:\n",
    "            param = json.load(outfile)\n",
    "            teacher.epsilon = param.get('epsilon')\n",
    "    \n",
    "    if LOAD_MODEL_FROM_FILE:\n",
    "        agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pkl\"))\n",
    "        attacker.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+ \"_atk_\" +\".pkl\"))\n",
    "\n",
    "        with open(MODEL_PATH+str(LOAD_FILE_EPISODE)+'.json') as outfile:\n",
    "            param = json.load(outfile)\n",
    "            agent.epsilon = param.get('epsilon')\n",
    "            attacker.epsilon = param.get('epsilon')\n",
    "\n",
    "        startEpisode = LOAD_FILE_EPISODE + 1\n",
    "\n",
    "    else:\n",
    "        startEpisode = 1\n",
    "\n",
    "    last_100_ep_reward = deque(maxlen=100)  # Last 100 episode rewards\n",
    "    total_step = 1  # Cumulkative sum of all steps in episodes\n",
    "    ATT_FLAG = False\n",
    "    proxy_memory = deque(maxlen=BATCH_SIZE+1)\n",
    "    teacher_act_list = list()\n",
    "    for episode in range(startEpisode, MAX_EPISODE):\n",
    "\n",
    "        startTime = time.time()  # Keep time\n",
    "        state = environment.reset()  # Reset env\n",
    "\n",
    "        state = agent.preProcess(state)  # Process image\n",
    "\n",
    "        # Stack state . Every state contains 4 time contionusly frames\n",
    "        # We stack frames like 4 channel image\n",
    "        state = np.stack((state, state, state, state))\n",
    "\n",
    "        total_max_q_val = 0  # Total max q vals\n",
    "        total_reward = 0  # Total reward for each episode\n",
    "        total_loss = 0  # Total loss for each episode\n",
    "        num_action = 0\n",
    "        count_target_state = 0\n",
    "        reward_count = 0\n",
    "        reward_state_count = 0.001\n",
    "        for step in range(MAX_STEP):\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = agent.act(state)  # Act\n",
    "            \n",
    "            # check the action of an optimal Q network at current state and compare with learner's action\n",
    "            teacher_action = teacher.act(state)\n",
    "            if (teacher_action != 0):\n",
    "                count_target_state += 1\n",
    "                if (action == 0):\n",
    "                    num_action += 1\n",
    "    \n",
    "            next_state, reward, done, info = environment.step(action)  # Observe\n",
    "\n",
    "            next_state = agent.preProcess(next_state)  # Process image\n",
    "\n",
    "            # Stack state . Every state contains 4 time contionusly frames\n",
    "            # We stack frames like 4 channel image\n",
    "            next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
    "            \n",
    "            \n",
    "            # Attacker perform it's action by publishing a reward stored in it's disk\n",
    "            return_reward = attacker.step_one(0, state, next_state, action, reward, done)\n",
    "            \n",
    "            # Learner uses the publish reward and store the latest transition tuple in it's replay buffer\n",
    "            if return_reward is not None:\n",
    "                    # Store the poisoned transition in memory\n",
    "                    agent.storeResults(state, action, return_reward, next_state, done)  # Store to mem\n",
    "                \n",
    "                    # Store the latest transition tuple and teacher's action to a proxy memory to be used later to compute attacker's reward\n",
    "                    proxy_memory.append((state, action, return_reward, next_state, done))\n",
    "                    teacher_act_list.append(teacher_action)\n",
    "                    \n",
    "            else:\n",
    "                #pass\n",
    "                # Not used after intial few steps of learner's interaction with the environment\n",
    "                agent.storeResults(state, action, 0, next_state, done)  # Store to mem\n",
    "                proxy_memory.append((state, action, 0, next_state, done))\n",
    "                teacher_act_list.append(teacher_action)\n",
    "\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state  # Update state\n",
    "\n",
    "            # Main Training Loop\n",
    "            if TRAIN_MODEL:\n",
    "                # Agent extract Batch data from replay buffer\n",
    "                loss, max_q_val, state_Q_val, update_agent, state_inf, next_state_inf, action_inf, reward_inf, done_inf = agent.extract_data(agent)  \n",
    "                \n",
    "                # check whether buffer size is more than min_replay_buffer size, if not, then don't start training\n",
    "                if update_agent == False:\n",
    "                    pass\n",
    "                # train\n",
    "                else:\n",
    "                    \n",
    "                    # Agent model in trained and updated with the extracted data from buffer\n",
    "                    loss, max_q_val = agent.train(agent, state_inf, next_state_inf, action_inf, reward_inf, done_inf)\n",
    "                     \n",
    "            else:\n",
    "                loss, max_q_val = [0, 0]\n",
    "            # variables to print the output \n",
    "            total_loss += loss\n",
    "            total_max_q_val += max_q_val\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "            # update the greedy epsilon value adaptively \n",
    "            if total_step % 1000 == 0:\n",
    "                agent.adaptiveEpsilon()  # Decrase epsilon\n",
    "                attacker.adaptiveEpsilon()\n",
    "      \n",
    "            if done:  # Episode completed\n",
    "                currentTime = time.time()  # Keep current time\n",
    "                time_passed = currentTime - startTime  # Find episode duration\n",
    "                current_time_format = time.strftime(\"%H:%M:%S\", time.gmtime())  # Get current dateTime as HH:MM:SS\n",
    "                epsilonDict = {'epsilon': agent.epsilon}  # Create epsilon dict to save model as file\n",
    "\n",
    "                # Save the models attacker and learner after completion of Save_model_interval number of episodes\n",
    "                if SAVE_MODELS and episode % SAVE_MODEL_INTERVAL == 0:  # Save model as file\n",
    "                    weightsPath = MODEL_PATH + str(episode) + '.pkl'\n",
    "                    weightsPath_atk = MODEL_PATH + str(episode) + \"_atk_\" + '.pkl'\n",
    "                    epsilonPath = MODEL_PATH + str(episode) + '.json'\n",
    "\n",
    "                    torch.save(agent.online_model.state_dict(), weightsPath)\n",
    "                    torch.save(attacker.online_model.state_dict(), weightsPath_atk)\n",
    "                    with open(epsilonPath, 'w') as outfile:\n",
    "                        json.dump(epsilonDict, outfile)\n",
    "                # After every episode update the target network of learner and attacker\n",
    "                if TRAIN_MODEL:\n",
    "                    agent.target_model.load_state_dict(agent.online_model.state_dict())  # Update target model\n",
    "                    attacker.target_model.load_state_dict(attacker.online_model.state_dict())\n",
    "                # Logging\n",
    "                last_100_ep_reward.append(total_reward)\n",
    "                avg_max_q_val = total_max_q_val / step\n",
    "                # printing values\n",
    "                outStr = \"Episode:{} Time:{} Reward:{:.2f} Loss:{:.2f} Last_100_Avg_Rew:{:.3f} Avg_Max_Q:{:.3f} Epsilon:{:.2f} Duration:{:.2f} Step:{} CStep:{} action_num:{} frac:{} \".format(\n",
    "                    episode, current_time_format, total_reward, total_loss, np.mean(last_100_ep_reward), avg_max_q_val, agent.epsilon, time_passed, step, total_step, num_action, num_action/count_target_state,\n",
    "                )\n",
    "\n",
    "                print(outStr)\n",
    "\n",
    "                #if SAVE_MODELS:\n",
    "                if True:\n",
    "                    outputPath = MODEL_PATH + \"out\" + '.txt'  # Save outStr to file\n",
    "                    with open(outputPath, 'a') as outfile:\n",
    "                        outfile.write(outStr+\"\\n\")\n",
    "\n",
    "                break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reward",
   "language": "python",
   "name": "reward"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
